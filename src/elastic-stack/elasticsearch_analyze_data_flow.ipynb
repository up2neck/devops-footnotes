{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db05ecc-e387-42bf-8744-67634b325165",
   "metadata": {},
   "source": [
    "# Analyze ingest data flow within Elasticsearch\n",
    "\n",
    "The notebook provides insights to ingest pipelines flow for Elasticsearch cluster.\n",
    "\n",
    "It might be useful to deobfuscate data flow for further refactoring of ingest pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b12348-f2c6-42a7-906c-604c3330b3e5",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afe2ba-fe85-4942-a2d5-fda629c54be3",
   "metadata": {},
   "source": [
    "### Install required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b3356-2899-45be-9870-8b6d13b40518",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas~=2.2 elasticsearch~=8.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4f8ef-a04c-4237-8251-2a012018ee6f",
   "metadata": {},
   "source": [
    "### Restart Jupiter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b0bdd-e783-4914-9bd5-5c9acb4559ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffba170-2093-40cb-817b-85daaffd11f2",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e9966-0bc6-45eb-b8e4-90c0757e4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch, NotFoundError\n",
    "from IPython.display import display, FileLink\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb03a8c-7acf-4559-9f28-ff40a72022b6",
   "metadata": {},
   "source": [
    "### Input Elasticsearch connection settings\n",
    "\n",
    "To connect Elasticsearh instance it's hostname and valid API key are required.\n",
    "\n",
    "API key can be created via Kibana - [https://www.elastic.co/guide/en/kibana/current/api-keys.html](https://www.elastic.co/guide/en/kibana/current/api-keys.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa91154-1da4-4cc0-ab5a-808b6fe8aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_host = input(\"Enter Elasticsearch hostname: \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7364e6e-be66-4631-ad97-5c6b3b824374",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_api_key = getpass.getpass(\"Enter Elasticsearch API key:\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d9bb0-7a33-4363-91c5-ee21c9aa1b7f",
   "metadata": {},
   "source": [
    "### Create Elasticsearch client and connect the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4b48d-2b1b-4ec2-af48-e5c1e766818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Elasticsearch(\n",
    "    hosts=elasticsearch_host,\n",
    "    api_key=elasticsearch_api_key,\n",
    "    verify_certs=False,             # Elasticsearch certificate is signed by the non-public authority, so ignore any warning\n",
    "    ssl_show_warn=False             # Unverified SSL/TLS connections cause a lot of warnings, so them should be supressed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a53036-4322-410b-9606-a254a2351c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.cat.health())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d1322-083a-455a-9542-c66549cebb52",
   "metadata": {},
   "source": [
    "## Analyze incoming data flow\n",
    "\n",
    "When data arrives at Elasticsearch, its processing flow is governed by the following steps:\n",
    "\n",
    "1. **Index Template Matching**\n",
    "- If the target index does not exist, Elasticsearch matches the target index name against the index patterns defined in the index templates.\n",
    "- The index template with the highest priority and a matching index pattern is selected to create the new index where the data will be indexed.\n",
    "\n",
    "2. **Ingest Pipeline Reference**\n",
    "- The incoming data can be directed through an ingest pipeline, which may be specified in a `/bulk` request (a write request to Elasticsearch) or directly within the index itself.\n",
    "\n",
    "3. **Initial Data Processing**\n",
    "- The data is first processed by the default ingest pipeline's processors before it is indexed.\n",
    "\n",
    "4. **Pipeline Rerouting and Reindexing**\n",
    "- An ingest pipeline can contain references to another ingest pipeline or include a reroute processor, which can redirect the data to a different index than initially defined.\n",
    "- This rerouting allows the data to be processed according to the initial ingest pipeline and then potentially redirected and processed through the ingest pipeline of the destination index.\n",
    "- Such behavior can complicate the data flow and lead to challenges in tracking and accountability of the incoming data.\n",
    "\n",
    "5. **Final Ingest Pipeline Processing**\n",
    "- If a final ingest pipeline is defined, the data passes through its processors before being indexed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e4b74-f64a-4dfb-8e44-feae5081d1c7",
   "metadata": {},
   "source": [
    "### Analyze flow for indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162beef-c849-4558-8e74-642429cd05b8",
   "metadata": {},
   "source": [
    "#### Process description\n",
    "1. **Retrieve Index Settings**:\n",
    "   - Obtain the index settings from Elasticsearch to identify the `default_pipeline` and `final_pipeline` settings. These settings determine the pipelines that will process the data during indexing.\n",
    "\n",
    "2. **Simulate Index API Usage**:\n",
    "   - If the destination index does not exist, utilize the Simulate Index API to test and validate the pipeline configurations. More details can be found in the Elasticsearch documentation: [Simulate Index API](https://www.elastic.co/guide/en/elasticsearch/reference/current/simulate-index-api.html).\n",
    "\n",
    "3. **Concatenate pipeline steps**:\n",
    "   - Steps in pipelines are concatenated in the single list to represent them as CSV file\n",
    "   - If step includes a **pipeline** processor the referenced pipeline is also traversed\n",
    "   - **WARNING: reroute** processor is not supported yet   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6a59b-0b1c-41f4-80d3-18ef3e37c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "def get_index_ingest_pipelines(client, index_pattern):\n",
    "    \"\"\"\n",
    "    Retrieve ingest pipelines associated with an index pattern.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.indices.get_settings(index=index_pattern)\n",
    "    except NotFoundError:\n",
    "        logging.warning(\"Matching index does not exist. Using the Simulate index API to simulate pipeline index settings.\")\n",
    "        simulate_response = client.indices.simulate_index_template(name=index_pattern)\n",
    "        response = {index_pattern: simulate_response.get(\"template\")}\n",
    "\n",
    "    if len(response) > 1:\n",
    "        logging.warning(f\"Return values contain {len(res_body)} indices; this might be a data stream or an index pattern.\")\n",
    "\n",
    "    pipelines = []\n",
    "    for index, content in response.items():\n",
    "        settings = content.get(\"settings\", {}).get(\"index\", {})\n",
    "        pipelines.append({\n",
    "            \"index\": index,\n",
    "            \"default_pipeline\": settings.get(\"default_pipeline\"),\n",
    "            \"final_pipeline\": settings.get(\"final_pipeline\"),\n",
    "        })\n",
    "\n",
    "    return pipelines\n",
    "\n",
    "def traverse_pipeline(client, pipeline_id):\n",
    "    \"\"\"\n",
    "    Recursively traverse and collect details of an ingest pipeline.\n",
    "    \"\"\"\n",
    "    if not pipeline_id:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        pipeline = client.ingest.get_pipeline(id=pipeline_id).get(pipeline_id, {})\n",
    "    except NotFoundError:\n",
    "        return [{\"pipeline_id\": pipeline_id, \"error\": \"Pipeline does not exist\"}]\n",
    "\n",
    "    journey = []\n",
    "    for proc in pipeline.get(\"processors\", []):\n",
    "        proc_type = next(iter(proc), \"undefined\")\n",
    "        journey.append({\"pipeline_id\": pipeline_id, \"processor_type\": proc_type, \"processor_config\": proc[proc_type]})\n",
    "\n",
    "        if \"pipeline\" in proc:\n",
    "            journey.extend(traverse_pipeline(client, proc[\"pipeline\"].get(\"name\")))\n",
    "\n",
    "    return journey\n",
    "\n",
    "# Read index pattern from input\n",
    "index_pattern = input(\"Input index name to analyze the data flow: \").strip()\n",
    "\n",
    "pipelines = get_index_ingest_pipelines(client, index_pattern)\n",
    "\n",
    "# Traverse and display\n",
    "for pipeline_info in pipelines:\n",
    "    index = pipeline_info[\"index\"]\n",
    "    flow = traverse_pipeline(client, pipeline_info[\"default_pipeline\"])\n",
    "    flow.extend(traverse_pipeline(client, pipeline_info[\"final_pipeline\"]))\n",
    "\n",
    "    df = pd.DataFrame(flow)\n",
    "    \n",
    "    # Save DataFrame to CSV\n",
    "    output_dir = Path('temp')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_path = output_dir / f\"data_flow_{index}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Display a link to download the CSV\n",
    "    display(FileLink(csv_path, result_html_prefix=\"Open CSV file: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccc499-53ae-4422-b0b6-dd9f8d10feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
